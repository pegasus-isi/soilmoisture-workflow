{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "intro-md",
   "source": [
    "# Soil Moisture Analysis & Irrigation Prediction Pegasus Workflow\n",
    "\n",
    "Precision irrigation is critical for sustainable agriculture, water conservation, and crop yield optimization. The [Open-Meteo](https://open-meteo.com/) platform provides free access to historical weather and soil data from ERA5-Land reanalysis, covering global soil moisture and temperature at hourly resolution.\n",
    "\n",
    "This workflow fetches soil moisture and temperature data for user-defined field polygons, analyzes moisture levels with crop-specific and soil-specific thresholds, trains an **LSTM neural network** for 24-hour soil moisture forecasting, generates irrigation recommendations combining ML predictions with rule-based decision logic, and produces multi-panel visualizations.\n",
    "\n",
    "## Containers\n",
    "\n",
    "All tools required to execute the jobs are included in a single container available on DockerHub:\n",
    "\n",
    "[Soil Moisture Container](https://hub.docker.com/r/kthare10/soilmoisture) defined in `Docker/SoilMoisture_Dockerfile` with:\n",
    "* pandas, numpy, matplotlib, scipy, requests (base analysis)\n",
    "* torch, scikit-learn, tqdm (ML forecasting)\n",
    "* pytz (timezone handling)\n",
    "\n",
    "## Accessing the Input Data\n",
    "\n",
    "Soil data is fetched from the **Open-Meteo Historical Weather API** (`https://archive-api.open-meteo.com/v1/era5`), which provides ERA5-Land reanalysis data from ECMWF. **No API key is required.** The API returns hourly soil moisture (0-7 cm) and soil temperature (0-7 cm, 7-28 cm) for polygon centroids.\n",
    "\n",
    "Field locations are defined in a **polygons JSON file** with latitude/longitude coordinates.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "The workflow processes each field polygon through a 5-step pipeline:\n",
    "\n",
    "| Job Label              | Description                                                                          |\n",
    "|------------------------|--------------------------------------------------------------------------------------|\n",
    "| fetch_soil_data        | Fetches hourly soil moisture and temperature from Open-Meteo for polygon centroids   |\n",
    "| analyze_moisture       | Computes moisture statistics, water deficit, trend analysis, crop/soil thresholds     |\n",
    "| train_model            | Trains 2-layer LSTM (64 hidden units) for 24-hour soil moisture forecasting          |\n",
    "| predict_irrigation     | Generates irrigation recommendations using ML predictions + rule-based urgency logic |\n",
    "| visualize_moisture     | Creates multi-panel dashboard: time series, urgency gauge, water status, trend, recs  |\n",
    "\n",
    "### Dependency Graph\n",
    "\n",
    "```\n",
    "fetch_soil_data → analyze_moisture ──→ predict_irrigation → visualize_moisture\n",
    "       └──────→ train_model ─────────↗\n",
    "```\n",
    "\n",
    "### Supported Crops & Soil Types\n",
    "\n",
    "**Crops:** tomato, corn, wheat, lettuce, potato, grape, alfalfa, cotton (each with specific wilting/stress/optimal thresholds)\n",
    "\n",
    "**Soils:** sand, sandy_loam, loam, clay_loam, clay (each with specific field capacity and wilting point values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "create-wf-md",
   "source": [
    "## 1. Create the Soil Moisture Workflow\n",
    "\n",
    "First, configure the field polygon(s), date range, crop type, soil type, and ML training parameters.\n",
    "\n",
    "The polygons JSON file should define field boundaries as coordinate arrays. Example format:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": \"field1\",\n",
    "    \"name\": \"North Field\",\n",
    "    \"coordinates\": [[-121.50, 37.50], [-121.48, 37.50], [-121.48, 37.52], [-121.50, 37.52], [-121.50, 37.50]]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Then the workflow class below will:\n",
    "1. Build the Pegasus catalogs (sites, transformations, replicas)\n",
    "2. Construct the DAG with fetch, analyze, train, predict, and visualize jobs for each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "config-cell",
   "outputs": [],
   "source": [
    "# Path to polygons JSON file defining field boundaries\n",
    "POLYGONS_FILE = 'polygons.json'\n",
    "\n",
    "# Polygon IDs to analyze (set to None to use all polygons in the file)\n",
    "POLYGON_IDS = ['field1']\n",
    "\n",
    "# Date range for soil data (Open-Meteo provides historical ERA5-Land data)\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-01-31'\n",
    "\n",
    "# Crop type for threshold selection\n",
    "# Choices: tomato, corn, wheat, lettuce, potato, grape, alfalfa, cotton, default\n",
    "CROP_TYPE = 'tomato'\n",
    "\n",
    "# Soil type for water capacity calculation\n",
    "# Choices: sand, sandy_loam, loam, clay_loam, clay, default\n",
    "SOIL_TYPE = 'loam'\n",
    "\n",
    "# LSTM training epochs (more epochs = better model but longer training)\n",
    "ML_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "note-md",
   "source": [
    "**Note:** At least 31 days of data is recommended for LSTM training (minimum ~58 hourly records needed to create 10 training sequences). Longer periods improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "workflow-cell",
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Import Pegasus API ---\n",
    "from Pegasus.api import *\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# --- Main workflow class ---\n",
    "class SoilMoistureWorkflow():\n",
    "    wf = None\n",
    "    sc = None\n",
    "    tc = None\n",
    "    rc = None\n",
    "    props = None\n",
    "\n",
    "    dagfile = None\n",
    "    wf_dir = None\n",
    "    shared_scratch_dir = None\n",
    "    local_storage_dir = None\n",
    "    wf_name = \"soilmoisture\"\n",
    "\n",
    "    # --- Init ---\n",
    "    def __init__(self, dagfile=\"workflow.yml\"):\n",
    "        self.dagfile = dagfile\n",
    "        self.wf_dir = str(Path(\".\").resolve())\n",
    "        self.shared_scratch_dir = os.path.join(self.wf_dir, \"scratch\")\n",
    "        self.local_storage_dir = os.path.join(self.wf_dir, \"output\")\n",
    "\n",
    "    # --- Write files in directory ---\n",
    "    def write(self):\n",
    "        if self.sc is not None:\n",
    "            self.sc.write()\n",
    "        self.props.write()\n",
    "        self.rc.write()\n",
    "        self.tc.write()\n",
    "\n",
    "        try:\n",
    "            self.wf.write(file=self.dagfile)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Plan and Submit the workflow ---\n",
    "    def plan_submit(self):\n",
    "        try:\n",
    "            self.wf.plan(submit=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Get status of the workflow ---\n",
    "    def status(self):\n",
    "        try:\n",
    "            self.wf.status(long=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Wait for the workflow to finish ---\n",
    "    def wait(self):\n",
    "        try:\n",
    "            self.wf.wait()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Get statistics of the workflow ---\n",
    "    def statistics(self):\n",
    "        try:\n",
    "            self.wf.statistics()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Configuration (Pegasus Properties) ---\n",
    "    def create_pegasus_properties(self):\n",
    "        self.props = Properties()\n",
    "        self.props[\"pegasus.transfer.threads\"] = \"16\"\n",
    "        return\n",
    "\n",
    "    # --- Site Catalog ---\n",
    "    def create_sites_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.sc = SiteCatalog()\n",
    "\n",
    "        local = (Site(\"local\")\n",
    "                    .add_directories(\n",
    "                        Directory(Directory.SHARED_SCRATCH, self.shared_scratch_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.shared_scratch_dir, Operation.ALL)),\n",
    "                        Directory(Directory.LOCAL_STORAGE, self.local_storage_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.local_storage_dir, Operation.ALL))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        exec_site = (Site(exec_site_name)\n",
    "                        .add_condor_profile(universe=\"vanilla\")\n",
    "                        .add_pegasus_profile(style=\"condor\")\n",
    "                    )\n",
    "\n",
    "        self.sc.add_sites(local, exec_site)\n",
    "\n",
    "    # --- Transformation Catalog (Executables and Containers) ---\n",
    "    def create_transformation_catalog(self, exec_site_name=\"condorpool\",\n",
    "                                       container_image=\"kthare10/soilmoisture:latest\"):\n",
    "        self.tc = TransformationCatalog()\n",
    "\n",
    "        # Container\n",
    "        soilmoisture_container = Container(\"soilmoisture_container\",\n",
    "            container_type=Container.SINGULARITY,\n",
    "            image=f\"docker://{container_image}\",\n",
    "            image_site=\"docker_hub\"\n",
    "        )\n",
    "\n",
    "        # Transformations\n",
    "        fetch_soil_data = Transformation(\"fetch_soil_data\", site=exec_site_name,\n",
    "            pfn=os.path.join(self.wf_dir, \"fetch_soil_data.py\"),\n",
    "            is_stageable=True, container=soilmoisture_container\n",
    "        ).add_pegasus_profile(memory=\"1 GB\")\n",
    "\n",
    "        analyze_moisture = Transformation(\"analyze_moisture\", site=exec_site_name,\n",
    "            pfn=os.path.join(self.wf_dir, \"bin/analyze_moisture.py\"),\n",
    "            is_stageable=True, container=soilmoisture_container\n",
    "        ).add_pegasus_profile(memory=\"1 GB\")\n",
    "\n",
    "        train_model = Transformation(\"train_model\", site=exec_site_name,\n",
    "            pfn=os.path.join(self.wf_dir, \"bin/train_model.py\"),\n",
    "            is_stageable=True, container=soilmoisture_container\n",
    "        ).add_pegasus_profile(memory=\"4 GB\")\n",
    "\n",
    "        predict_irrigation = Transformation(\"predict_irrigation\", site=exec_site_name,\n",
    "            pfn=os.path.join(self.wf_dir, \"bin/predict_irrigation.py\"),\n",
    "            is_stageable=True, container=soilmoisture_container\n",
    "        ).add_pegasus_profile(memory=\"2 GB\")\n",
    "\n",
    "        visualize_moisture = Transformation(\"visualize_moisture\", site=exec_site_name,\n",
    "            pfn=os.path.join(self.wf_dir, \"bin/visualize_moisture.py\"),\n",
    "            is_stageable=True, container=soilmoisture_container\n",
    "        ).add_pegasus_profile(memory=\"2 GB\")\n",
    "\n",
    "        self.tc.add_containers(soilmoisture_container)\n",
    "        self.tc.add_transformations(\n",
    "            fetch_soil_data, analyze_moisture, train_model,\n",
    "            predict_irrigation, visualize_moisture\n",
    "        )\n",
    "\n",
    "    # --- Replica Catalog ---\n",
    "    def create_replica_catalog(self):\n",
    "        self.rc = ReplicaCatalog()\n",
    "\n",
    "    # --- Create Workflow ---\n",
    "    def create_workflow(self, args):\n",
    "        self.wf = Workflow(self.wf_name)\n",
    "\n",
    "        polygons_file = File(os.path.basename(args.polygons_file))\n",
    "        self.rc.add_replica(\"local\", polygons_file, os.path.abspath(args.polygons_file))\n",
    "\n",
    "        # Shared ML model files\n",
    "        model_file = File(\"soil_moisture_model.pt\")\n",
    "        model_metadata = File(\"soil_moisture_model_metadata.json\")\n",
    "\n",
    "        # Collect all fetch and analyze jobs\n",
    "        fetch_jobs = []\n",
    "        analyze_jobs = []\n",
    "        soil_data_files = []\n",
    "\n",
    "        # Add fetch and analyze jobs for each polygon\n",
    "        for polygon_id in args.polygon_ids:\n",
    "            fetch_job, analyze_job, soil_data_file = self._add_fetch_analyze_jobs(\n",
    "                polygon_id, args, polygons_file\n",
    "            )\n",
    "            fetch_jobs.append(fetch_job)\n",
    "            analyze_jobs.append(analyze_job)\n",
    "            soil_data_files.append(soil_data_file)\n",
    "\n",
    "        # Add ML training job (uses first polygon's data)\n",
    "        train_job = self._add_train_job(soil_data_files, model_file, model_metadata, args)\n",
    "\n",
    "        # Training depends on having fetched data\n",
    "        self.wf.add_dependency(fetch_jobs[0], children=[train_job])\n",
    "\n",
    "        # Add predict and visualize jobs for each polygon (uses trained model)\n",
    "        for i, polygon_id in enumerate(args.polygon_ids):\n",
    "            self._add_predict_visualize_jobs(\n",
    "                polygon_id, args,\n",
    "                soil_data_files[i], analyze_jobs[i],\n",
    "                model_file, model_metadata, train_job\n",
    "            )\n",
    "\n",
    "    def _add_fetch_analyze_jobs(self, polygon_id, args, polygons_file):\n",
    "        soil_data_file = File(f\"{polygon_id}_soil_data.csv\")\n",
    "        analysis_file = File(f\"{polygon_id}_analysis.json\")\n",
    "\n",
    "        # Job 1: Fetch soil data\n",
    "        fetch_job = Job(\"fetch_soil_data\", _id=f\"fetch_{polygon_id}\", node_label=f\"fetch_{polygon_id}\")\n",
    "        fetch_job.add_args(\"--fetch\", \"--polygon-id\", polygon_id)\n",
    "        fetch_job.add_args(\"--start-date\", args.start_date, \"--end-date\", args.end_date)\n",
    "        fetch_job.add_args(\"--output\", soil_data_file)\n",
    "        fetch_job.add_args(\"--polygons-file\", polygons_file)\n",
    "        fetch_job.add_inputs(polygons_file)\n",
    "        fetch_job.add_outputs(soil_data_file, stage_out=True, register_replica=False)\n",
    "        fetch_job.add_pegasus_profile(label=polygon_id)\n",
    "\n",
    "        # Job 2: Analyze moisture\n",
    "        analyze_job = Job(\"analyze_moisture\", _id=f\"analyze_{polygon_id}\", node_label=f\"analyze_{polygon_id}\")\n",
    "        analyze_job.add_args(\n",
    "            \"--input\", soil_data_file,\n",
    "            \"--output\", analysis_file,\n",
    "            \"--crop-type\", args.crop_type,\n",
    "            \"--soil-type\", args.soil_type,\n",
    "        )\n",
    "        analyze_job.add_inputs(soil_data_file)\n",
    "        analyze_job.add_outputs(analysis_file, stage_out=True, register_replica=False)\n",
    "        analyze_job.add_pegasus_profile(label=polygon_id)\n",
    "\n",
    "        self.wf.add_jobs(fetch_job, analyze_job)\n",
    "        self.wf.add_dependency(fetch_job, children=[analyze_job])\n",
    "\n",
    "        return fetch_job, analyze_job, soil_data_file\n",
    "\n",
    "    def _add_train_job(self, soil_data_files, model_file, model_metadata, args):\n",
    "        primary_data_file = soil_data_files[0]\n",
    "\n",
    "        train_job = Job(\"train_model\", _id=\"train_ml_model\", node_label=\"train_ml_model\")\n",
    "        train_job.add_args(\n",
    "            \"--input\", primary_data_file,\n",
    "            \"--output\", model_file,\n",
    "            \"--metadata\", model_metadata,\n",
    "            \"--sequence-length\", \"24\",\n",
    "            \"--forecast-horizon\", \"24\",\n",
    "            \"--epochs\", str(args.ml_epochs),\n",
    "        )\n",
    "        train_job.add_inputs(primary_data_file)\n",
    "        train_job.add_outputs(model_file, stage_out=True, register_replica=False)\n",
    "        train_job.add_outputs(model_metadata, stage_out=True, register_replica=False)\n",
    "        train_job.add_pegasus_profile(label=\"ml_training\")\n",
    "\n",
    "        self.wf.add_jobs(train_job)\n",
    "        return train_job\n",
    "\n",
    "    def _add_predict_visualize_jobs(self, polygon_id, args,\n",
    "                                     soil_data_file, analyze_job,\n",
    "                                     model_file, model_metadata, train_job):\n",
    "        analysis_file = File(f\"{polygon_id}_analysis.json\")\n",
    "        prediction_file = File(f\"{polygon_id}_prediction.json\")\n",
    "        visualization_file = File(f\"{polygon_id}_visualization.png\")\n",
    "\n",
    "        # Job 4: Predict irrigation (with ML model)\n",
    "        predict_job = Job(\"predict_irrigation\", _id=f\"predict_{polygon_id}\", node_label=f\"predict_{polygon_id}\")\n",
    "        predict_job.add_args(\n",
    "            \"--analysis\", analysis_file,\n",
    "            \"--output\", prediction_file,\n",
    "            \"--model\", model_file,\n",
    "            \"--model-metadata\", model_metadata,\n",
    "            \"--soil-data\", soil_data_file,\n",
    "        )\n",
    "        predict_job.add_inputs(analysis_file, model_file, model_metadata, soil_data_file)\n",
    "        predict_job.add_outputs(prediction_file, stage_out=True, register_replica=False)\n",
    "        predict_job.add_pegasus_profile(label=polygon_id)\n",
    "\n",
    "        # Job 5: Visualize\n",
    "        visualize_job = Job(\"visualize_moisture\", _id=f\"visualize_{polygon_id}\", node_label=f\"visualize_{polygon_id}\")\n",
    "        visualize_job.add_args(\n",
    "            \"--data\", soil_data_file,\n",
    "            \"--analysis\", analysis_file,\n",
    "            \"--prediction\", prediction_file,\n",
    "            \"--output\", visualization_file,\n",
    "        )\n",
    "        visualize_job.add_inputs(soil_data_file, analysis_file, prediction_file)\n",
    "        visualize_job.add_outputs(visualization_file, stage_out=True, register_replica=False)\n",
    "        visualize_job.add_pegasus_profile(label=polygon_id)\n",
    "\n",
    "        self.wf.add_jobs(predict_job, visualize_job)\n",
    "\n",
    "        # Dependencies\n",
    "        self.wf.add_dependency(analyze_job, children=[predict_job])\n",
    "        self.wf.add_dependency(train_job, children=[predict_job])\n",
    "        self.wf.add_dependency(predict_job, children=[visualize_job])\n",
    "\n",
    "\n",
    "# --- Build and generate the workflow ---\n",
    "# Resolve polygon IDs from file if not specified\n",
    "polygon_ids = POLYGON_IDS\n",
    "if polygon_ids is None:\n",
    "    with open(POLYGONS_FILE, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    polygons = data.get('polygons', data) if isinstance(data, dict) else data\n",
    "    polygon_ids = [p.get('id') for p in polygons if p.get('id')]\n",
    "    print(f\"Using all polygon IDs from file: {polygon_ids}\")\n",
    "\n",
    "# Create args namespace for workflow creation\n",
    "args = Namespace(\n",
    "    polygons_file=POLYGONS_FILE,\n",
    "    polygon_ids=polygon_ids,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    crop_type=CROP_TYPE,\n",
    "    soil_type=SOIL_TYPE,\n",
    "    ml_epochs=ML_EPOCHS\n",
    ")\n",
    "\n",
    "dagfile = 'workflow.yml'\n",
    "\n",
    "workflow = SoilMoistureWorkflow(dagfile=dagfile)\n",
    "\n",
    "print(\"Creating execution sites...\")\n",
    "workflow.create_sites_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating workflow properties...\")\n",
    "workflow.create_pegasus_properties()\n",
    "\n",
    "print(\"Creating transformation catalog...\")\n",
    "workflow.create_transformation_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating replica catalog...\")\n",
    "workflow.create_replica_catalog()\n",
    "\n",
    "print(\"Creating soil moisture workflow DAG...\")\n",
    "workflow.create_workflow(args)\n",
    "\n",
    "workflow.write()\n",
    "print(f\"\\nSoil Moisture Workflow has been generated!\")\n",
    "print(f\"  Polygons: {len(polygon_ids)} ({', '.join(polygon_ids)})\")\n",
    "print(f\"  Crop type: {CROP_TYPE}\")\n",
    "print(f\"  Soil type: {SOIL_TYPE}\")\n",
    "print(f\"  ML epochs: {ML_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "view-dag-md",
   "source": [
    "## View the Generated Workflow DAG\n",
    "\n",
    "Before submitting, we can visualize the workflow DAG using `pegasus-graphviz`. The graph shows the 5-step pipeline: fetch data feeds both analyze and train_model, which then converge at predict_irrigation before the final visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "graphviz-cell",
   "outputs": [],
   "source": [
    "!pegasus-graphviz -f workflow.yml --output workflow.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "display-dag-cell",
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='workflow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "plan-submit-md",
   "source": [
    "## 2. Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e. the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "plan-submit-cell",
   "outputs": [],
   "source": [
    "workflow.plan_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "status-md",
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python `Workflow` object to monitor the status of the workflow. It shows in detail the counts of jobs of each status and whether a job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "status-cell",
   "outputs": [],
   "source": [
    "workflow.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "wait-cell",
   "outputs": [],
   "source": [
    "workflow.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "pilots-md",
   "source": [
    "## 3. Statistics\n",
    "\n",
    "Depending on whether the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `workflow.analyze()` to get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistics from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "statistics-cell",
   "outputs": [],
   "source": [
    "workflow.statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "results-md",
   "source": [
    "## 4. Examining the Results\n",
    "\n",
    "Once the workflow has finished, we can look at the output directory for our results. The workflow produces the following outputs for each polygon:\n",
    "\n",
    "```\n",
    "output/\n",
    "├── <polygon>_soil_data.csv                    # Raw hourly soil moisture & temperature data\n",
    "├── <polygon>_analysis.json                    # Moisture statistics, deficit, trend, classification\n",
    "├── <polygon>_prediction.json                  # Irrigation recommendation with urgency score\n",
    "├── <polygon>_visualization.png                # Multi-panel irrigation decision dashboard\n",
    "├── soil_moisture_model.pt                     # Trained LSTM model weights (shared)\n",
    "└── soil_moisture_model_metadata.json          # Model config, scaler parameters (shared)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "list-output-cell",
   "outputs": [],
   "source": [
    "!ls -ltR output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "viz-md",
   "source": [
    "### Irrigation Decision Dashboard\n",
    "\n",
    "The multi-panel visualization includes: a soil moisture time series with crop-specific threshold zones (critical/stressed/low/optimal/high/saturated), an urgency gauge (0-100), water status bar chart comparing current vs. optimal vs. field capacity, a trend indicator with daily change rate, and a recommendation panel with action, timing, and irrigation amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "display-viz-cell",
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "viz_pngs = sorted(glob.glob(\"output/*_visualization.png\"))\n",
    "for png in viz_pngs:\n",
    "    print(f\"\\n{png}\")\n",
    "    display(Image(filename=png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "analysis-md",
   "source": [
    "### Moisture Analysis Summary\n",
    "\n",
    "The moisture analysis provides crop-specific threshold evaluation, water deficit calculations, saturation percentage, trend analysis (increasing/stable/decreasing), and classification distribution across the monitoring period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "display-analysis-cell",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "analysis_files = sorted(glob.glob(\"output/*_analysis.json\"))\n",
    "for af in analysis_files:\n",
    "    with open(af, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    metadata = data.get('metadata', {})\n",
    "    thresholds = data.get('crop_thresholds', {})\n",
    "    polygons = data.get('polygon_analyses', [])\n",
    "\n",
    "    print(f\"\\n--- {metadata.get('crop_type', 'unknown')} / {metadata.get('soil_type', 'unknown')} ---\")\n",
    "    print(f\"  Thresholds: wilting={thresholds.get('wilting')}, stress={thresholds.get('stress')}, \"\n",
    "          f\"optimal={thresholds.get('optimal_low')}-{thresholds.get('optimal_high')}, \"\n",
    "          f\"saturated={thresholds.get('saturated')}\")\n",
    "\n",
    "    for poly in polygons:\n",
    "        stats = poly.get('moisture_stats', {})\n",
    "        deficit = poly.get('water_deficit', {})\n",
    "        trend = poly.get('trend', {})\n",
    "\n",
    "        print(f\"\\n  Polygon: {poly.get('polygon_id', 'unknown')}\")\n",
    "        print(f\"    Classification:    {poly.get('moisture_classification', 'N/A')}\")\n",
    "        print(f\"    Current moisture:  {stats.get('current', 'N/A')} m\\u00b3/m\\u00b3\")\n",
    "        print(f\"    Mean moisture:     {stats.get('mean', 'N/A')} m\\u00b3/m\\u00b3\")\n",
    "        print(f\"    Range:             {stats.get('min', 'N/A')} - {stats.get('max', 'N/A')}\")\n",
    "        print(f\"    Saturation:        {deficit.get('saturation_percent', 'N/A')}%\")\n",
    "        print(f\"    Trend:             {trend.get('trend', 'N/A')} (rate: {trend.get('daily_change_rate', 'N/A')}/day)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "prediction-md",
   "source": [
    "### Irrigation Prediction Summary\n",
    "\n",
    "The irrigation prediction combines ML-based 24-hour soil moisture forecasting with rule-based urgency scoring. Urgency ranges from 0 (no action needed) to 100 (immediate irrigation required). Actions include: `monitor`, `schedule_irrigation`, `irrigate_soon`, and `irrigate_immediately`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "display-prediction-cell",
   "outputs": [],
   "source": [
    "prediction_files = sorted(glob.glob(\"output/*_prediction.json\"))\n",
    "for pf in prediction_files:\n",
    "    with open(pf, 'r') as f:\n",
    "        pred_data = json.load(f)\n",
    "\n",
    "    summary = pred_data.get('summary', {})\n",
    "    predictions = pred_data.get('predictions', [])\n",
    "\n",
    "    print(f\"\\nOverall: {summary.get('overall_action', 'N/A')}\")\n",
    "    print(f\"ML predictions made: {summary.get('ml_predictions_made', 0)}\")\n",
    "\n",
    "    for pred in predictions:\n",
    "        ml = pred.get('ml_insights', {})\n",
    "        print(f\"\\n  Polygon: {pred.get('polygon_id', 'unknown')}\")\n",
    "        print(f\"    Urgency score:     {pred.get('urgency_score', 'N/A')}/100\")\n",
    "        print(f\"    Action:            {pred.get('action', 'N/A')}\")\n",
    "        print(f\"    Timing:            {pred.get('recommended_timing', 'N/A')}\")\n",
    "        print(f\"    Irrigation amount: {pred.get('irrigation_amount_mm', 0)} mm\")\n",
    "        if ml.get('ml_prediction_available'):\n",
    "            print(f\"    ML predicted min:  {ml.get('predicted_min_moisture', 'N/A')}\")\n",
    "            print(f\"    ML predicted trend: {ml.get('predicted_trend', 'N/A')}\")\n",
    "            print(f\"    Hours to critical: {ml.get('hours_until_critical', 'N/A')}\")\n",
    "            print(f\"    Hours to stress:   {ml.get('hours_until_stress', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "model-md",
   "source": [
    "### Trained LSTM Model Info\n",
    "\n",
    "The LSTM model metadata includes architecture configuration, feature scaling parameters, and training information. The model is shared across all polygons in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "display-model-cell",
   "outputs": [],
   "source": [
    "model_meta_files = sorted(glob.glob(\"output/*_model_metadata.json\"))\n",
    "for mf in model_meta_files:\n",
    "    with open(mf, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    config = meta.get('config', {})\n",
    "    print(f\"Model type:        {meta.get('model_type', 'N/A')}\")\n",
    "    print(f\"Input features:    {config.get('input_size', 'N/A')} ({', '.join(config.get('feature_cols', []))})\")\n",
    "    print(f\"Hidden size:       {config.get('hidden_size', 'N/A')}\")\n",
    "    print(f\"LSTM layers:       {config.get('num_layers', 'N/A')}\")\n",
    "    print(f\"Sequence length:   {config.get('sequence_length', 'N/A')} hours\")\n",
    "    print(f\"Forecast horizon:  {config.get('forecast_horizon', 'N/A')} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
